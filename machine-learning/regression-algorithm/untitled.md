# Text

Partial Regression Plots and Correlations In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of thn multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?Partial Regression Plots and Correlations In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. T. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?Partial Regression Plots and Correlations In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?Partial Regression Plots and Correlations In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?Partial Regression Plots and Correlations In multiple linear regression there are several explanatory variables, and these will usually be correlated with each other. This leads us to ask how to attribute the contribution of each one toward explaining the outcome variable. In the low-birthweight infant data, for example, length is useful in explaining birth weight. But length and birth weight are also related to all of the other variables in the data. How can we separate the contributions made by each of these individual explanatory variables?Each individual observation is deleted in turn, and then a new model is fitted corresponding to each deleted observation. The omitted observation is then replaced before the next observation is deleted. Many different things can change, and a number of comparisons can be made between the model fitted with the full data and the model fitted from jackknifed data. The SAS program that produces these diagnostics is given in Table 5.4. Put these program lines after the data step of the program in Table 5.2. The / p influence partial options produce the fitted value, measures of influence, and partial residual plots that are described later, in Section 5.5. The program in Table 5.4 produces a new SAS dataset called diag that contains the diagnostic measures. This dataset is merged with the dataset containing the original data to produce a new dataset called combo. As with all programs that involve creating and modifying existing datasets, be sure to use proc print until you are certain that everything is working as you wanted it to.

In addition to the usual residual (observed minus expected), there is also a standardized student residual RStudent, which includes a measure of hat diagonal that takes into account the change in estimated regression coefficients when this observation is jackknifed. These residuals should be comparable to the usual residuals in most settings. As is the case with usual residuals, observations with values of RStudent greater than 2 in absolute value are worth noting

Another useful jackknife diagnostic is the change in the estimated regression coefficients (including the intercept) when an observation is deleted and the model is refitted. These are collectively called dfbeta. There will be a dfbeta for every observation and every regression parameter in your model. When you run the program in Table 5.4, you will see the list of these variables and how SAS names them. The dfbeta changes in the estimated regression coefficients are standardized to make them comparable, but, as with the other diagnostics described in this section, your visual judgment is probably the best way to examine these.

