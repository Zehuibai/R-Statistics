# Decision Tree Model

## Decision tree algorithm

Tree-based models are a class of nonparametric algorithms that work by partitioning the feature space into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. Predictions are obtained by fitting a simpler model (e.g., a constant like the average response value) in each region. 

> 基于树的模型是一类非参数算法，其通过使用一组拆分规则将特征空间划分为具有相似响应值的多个较小（非重叠）区域来工作。 通过在每个区域中拟合更简单的模型（例如，类似于平均响应值的常数）来获得预测。

{% embed url="https://www.cnblogs.com/pinard/p/6050306.html" %}

{% embed url="https://www.cnblogs.com/pinard/p/6053344.html" %}

####

### ID3 Algorithm 

1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。熵度量了事物的不确定性，**越不确定的事物，它的熵就越大**。具体的，随机变量X的熵的表达式如下：

$$
H(X) = -\sum\limits_{i=1}^{n}p_i logp_i
$$

* 其中n代表X的n种不同的离散取值
* $$p_i$$代表了X取值为i的概率
* log为以2或者e为底的对数。

熟悉了一个变量X的熵，很容易推广到多个个变量的联合熵，这里给出两个变量X和Y的联合熵表达式：

$$
H(X,Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i,y_i)
$$

有了联合熵，又可以得到条件熵的表达式H(X|Y)，条件熵类似于条件概率,它度量了我们的X在知道Y以后剩下的不确定性。表达式如下：

$$
H(X|Y) = -\sum\limits_{x_i \in X}\sum\limits_{y_i \in Y}p(x_i,y_i)logp(x_i|y_i) = \sum\limits_{j=1}^{n}p(y_j)H(X|y_j)
$$

* H(X)度量了X的不确定性
* 条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性
* **H(X)-H(X|Y):  **度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，，记为I(X,Y)。在决策树ID3算法中叫做**信息增益 **information divergence。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用~~来~~分类。

ID3算法就是用信息增益大小来判断当前节点应该用什么特征来构建决策树，用**计算出的信息增益最大的特征来建立决策树的当前节点**。

#### 举例: 

有15个样本D，输出为0或者1。其中有9个输出为1， 6个输出为0。 样本中有个特征A，取值为A1，A2和A3。在取值为A1的样本的输出中，有3个输出为1， 2个输出为0，取值为A2的样本输出中,2个输出为1,3个输出为0， 在取值为A3的样本中，4个输出为1，1个输出为0.

* 样本D的熵为： $$H(D) = -(\frac{9}{15}log_2\frac{9}{15} + \frac{6}{15}log_2\frac{6}{15}) = 0.971$$ 
* 样本D在特征下的条件熵为： $$H(D|A) = \frac{5}{15}H(D1) + \frac{5}{15}H(D2) + \frac{5}{15}H(D3)$$ $$= -\frac{5}{15}(\frac{3}{5}log_2\frac{3}{5} + \frac{2}{5}log_2\frac{2}{5}) - \frac{5}{15}(\frac{2}{5}log_2\frac{2}{5} + \frac{3}{5}log_2\frac{3}{5}) -\frac{5}{15}(\frac{4}{5}log_2\frac{4}{5} + \frac{1}{5}log_2\frac{1}{5}) = 0.888$$ 
* 对应的信息增益为 $$I(D,A) = H(D) - H(D|A) = 0.083$$ 

#### Algorithm process

输入的是m个样本，样本输出集合为D，每个样本有n个离散特征，特征集合即为A，输出为决策树T。

1. 初始化信息增益的间值 $$\epsilon$$
2. 判断样本是否为同一类输出 $$D_{i},$$ 如果是则返回单节点树T。标记类别为 $$D_{i}$$
3. 判断特征是否为空, 如果是则返回单节点树T， 标记类别为样本中输出类别D实例数最多的类别。
4. 计算A中的各个特征 (一共n个) 对输出D的信息增益, 选择信息增益最大的特征 $$A_{g}$$ 
5. 如果 $$A_{g}$$ 的信息增益小于间值 $$\epsilon,$$ 则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。
6. 如果 $$A_{g}$$ 的信息增益小于间值 $$\epsilon,$$ , 按特征 $$A_{g}$$ 的不同取值 $$A_{g i}$$ 将对应的样本输出D分成不同的类别 $$D_{i 。}$$ 每个类别产生一个子节点。对应特征值为 $$A_{g i}$$ 返回增加了节点的数T。
7. 对于所有的子节点, 令 $$D=D_{i}, A=A-\left\{A_{g}\right\}$$ 递归调用2-6步, 得到子树 $$T_{i}$$ 并返回。

#### 不足

1. ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。
2. ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，**取值比较多的特征比取值少的特征信息增益大**。
3. ID3算法对于缺失值的情况没有做考虑
4. 没有考虑过拟合的问题 Overfitting

### C4.5 Algorithm 

ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。昆兰在C4.5算法中改进了上述4个问题. ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法

第一个问题，不能处理连续特征， C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点. 其中第i个划分点$$T_i$$表示为 $$T_i = \frac{a_i+a_{i+1}}{2}$$ 。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为at,则小于at的值为类别1，大于at的值为类别2，这样我们就做到了连续特征的离散化。

第二个问题，信息增益作为标准容易偏向于取值较多的特征的问题。我们引入一个信息增益比的变量 $$I_R(X,Y)$$ ，它是信息增益和特征熵的比值。表达式如下

$$
I_R(D,A) = \frac{I(A,D)}{H_A(D)}
$$

其中D为样本特征输出的集合，A为样本特征，对于特征熵$$H_A(D)$$

$$
H_A(D) = -\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$

其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。|D|为样本个数。**特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题**。

第三个缺失值处理的问题，主要需要解决的是两个问题，

* 一是在样本某些特征缺失的情况下选择划分的属性，
  * 对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。
* 二是选定了划分属性，对于在该属性上缺失特征的样本的处理。
  * 将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。

第四个问题，C4.5引入了正则化系数进行初步的剪枝。

#### 不足

1. 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。(CART树: 主要采用的是后剪枝加上交叉验证选择最合适的决策树)
2. C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
3. C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
4. C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。

### CART Algorithm 

对于C4.5算法的不足，比如模型是用较为复杂的熵来度量，使用了相对较为复杂的多叉树，只能处理分类不能处理回归等。对于这些问题， CART算法大部分做了改进。

* 在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。
* 在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。
* CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。

具体的，在分类问题中，假设有K个类别，第k个类别的概率为 $$p_k$$ , 则基尼系数的表达式为：

$$
Gini(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2
$$

如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为： 

$$
Gini(p) = 2p(1-p)
$$

对于个给定的样本D,假设有K个类别, 第k个类别的数量为$$C_k$$,则样本D的基尼系数表达式为：

$$
Gini(D) = 1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$

对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：

$$
Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
$$

对于CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点. 其中第i个划分点$$T_i$$表示为 $$T_i = \frac{a_i+a_{i+1}}{2}$$ 。对于这m-1个点，分别计算以该点作为二元分类点时的基尼系数。选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为$$a_t$$则小于$$a_t$$的值为类别1，大于$$a_t$$的值为类别2，这样我们就做到了连续特征的离散化。

#### Classification tree

分类树与回归树的运行原理是一样的，区别在于决定分裂过程的不是RSS，而是**误差率**。误差率不是简单地由误分类的观测数除以总观测数算出。实际上，进行树分裂 时，误分类率本身可能会导致这样一种情况:  可以从下次分裂中获得一些有用信息，但误分类率却没有改善。

* 假设有一个节点N0，节点中有7个标号为No的观测和3个标号为Yes的观测，我们就可以说误 分类率为30%
* 另一种误差测量方式进行计算，这种方式称为**基尼指数**。 单个节点的基尼指数计算公式如下: \
  基尼指数=1-(类别1的概率)^2-(类别的概率)^2\
  在此：对于N0，基尼指数为1 - (0.7)2 - (0.3)2，等于0.42

假设将节点N0分裂成两个节点N1和N2，N1中有3个观测属于 类别1，没有属于类别2的观测; N2中有4个观测属于类别1，3个属于类别2。现在，树的这个分支 的整体的误分类率还是30% 整体的基尼指数： 

* 基尼指数(N1) = 1 - (3/3)^2 - (0/3)^2= 0 
* 基尼指数(N2) = 1 - (4/7)^2 - (3/7)^2= 0.49 
* 新基尼指数 = (N1比例×基尼指数(N1))+(N2比例×基尼指数(N2)) = (0.3×0) + (0.7×0.49)=0.343. 改善了模型的不纯度，将其从原来的0.42减小到0.343，误分类率却没有变化。rpart()包就是使用Gini指数测量误差的

#### 分类树建立算法的具体流程

算法输入是训练集D，基尼系数的阈值，样本个数阈值。算法从根节点开始，用训练集递归的建立CART树。

1. 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。
2. 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。
3. 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，缺失值的处理方法和C4.5算法里描述的相同。
4. 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.
5. 对左右的子节点递归的调用1-4步，生成决策树。

#### Regression tree

树方法的精髓就是划分特征，从第一次分裂开始就要考虑如何最大程度改善RSS，然 后持续进行二叉分裂，直到树结束。后面的划分并不作用于全体数据集，而仅作用于上次划分时 落到这个分支之下的那部分数据。这个自顶向下的过程被称为“递归划分”。这个过程是贪婪的:

> Greed 贪婪的含义是，**算法在每次分裂中都追求最大程度减少RSS，而不管以后的划分中表现如何**。这样做的结果是，你可能会生成一个带有无效分支的 树，尽管偏差很小，但是方差很大。为了避免这个问题，生成完整的树之后，你要**对树进行剪枝**， 得到最优的规模。

* 优点是可以处理高度非线性关系
* 首要的问题就是，一个观测被赋予所属终端节点的平均值，这会损害整体预测效果(高偏差)
* 如果你一直对数据进行划分，树的层次越来越深，这样可以达到低偏差的效果，但是高方差又成了问题 (可以用交叉验证来选择合适的深度)

#### 回归树建立算法的具体流程

CART回归树和CART分类树的建立和预测的区别主要有下面两点：

　　　　1)连续值的处理方法不同

　　　　2)决策树建立后做预测的方式不同。

对于连续值的处理，我们知道CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型，但是对于回归模型，我们使用了常见的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为：(其中，c1为D1数据集的样本输出均值，c2为D2数据集的样本输出均值。)

$$
\begin{equation}
SSE = \sum_{i \in R_1}\left(y_i - c_1\right)^2 + \sum_{i \in R_2}\left(y_i - c_2\right)^2
\end{equation}
$$

$$
\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]
$$

对于决策树建立后做预测的方式，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果。

### Pruning

CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用均方差，一个使用基尼系数。由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。

有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是**后剪枝法**，即先生成决策树，然后**产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果**，选择泛化能力最好的剪枝策略。也就是说，CART树的剪枝算法可以概括为两步，

1. 从原始决策树生成各种剪枝效果的决策树
2. 用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树

#### 剪枝的损失函数度量

在剪枝的过程中，对于任意的一刻子树$$T_,$$其损失函数为：

$$
C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|
$$

其中，α为正则化参数，这和线性回归的正则化一样。 $$C(T_t)$$ 为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|Tt|是子树T的叶子节点的数量.

* 当 $$\alpha = 0$$ 时，即没有正则化，原始的生成的CART树即为最优子树。
* 当 $$\alpha = \infty$$ ，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。
* 一般来说，$$\alpha$$**越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小**。对于固定的$$\alpha$$，一定存在使损失函数Cα(T)最小的唯一子树。

看过剪枝的损失函数度量后，我们再来看看剪枝的思路，对于位于节点t的任意一颗子树$$T_t$$，如果没有剪枝，它的损失是 $$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|$$, 如果将其剪掉，仅仅保留根节点，则损失是 $$C_{\alpha}(T) = C(T) + \alpha$$ . 当 $$\alpha = 0$$ 或者$$\alpha$$很小时， $$C_{\alpha}(T_t) < C_{\alpha}(T)$$ , 当$$\alpha$$增大到一定的程度时 $$C_{\alpha}(T_t) = C_{\alpha}(T)$$ 

#### 算法主要过程如下

输入是CART树建立算法得到的原始决策树 $$T_{0}$$ 输出是最优决策子树 $$T_{\alpha 。}$$

1. 初始化 $$k=0, T=T_{0},$$ 最优子树集合 $$\omega=\{T\}_{\circ}$$
2. $$\alpha_{\min }=\infty$$
3. 从叶子节点开始自下而上计算各内部节点t的训练误差损失函数 $$C_{\alpha}\left(T_{t}\right)$$ (回归树为均方差, 分类树为基尼系数)，叶子节点数 $$\left|T_{t}\right|,$$ 以及正则化间值 $$\alpha=\min \left\{\frac{C(T)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}, \alpha_{\min }\right\},$$ 更新 $$\alpha_{\min }=\alpha$$
4. $$\alpha_{k}=\alpha_{\min }$$
5. 自上而下的访问子树的内部节点, 如果 $$\frac{C(T)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \leq \alpha_{k}$$ 时, 进行剪枝。并决定叶节点$$t$$的值。如果是分类树, 则是概率最高的类别, 如果是回归树，则是所有样本输出的均值。这样得到 $$\alpha_{k}$$ 对应的最优子树 $$T_{k}$$
6. 最优子树集合 $$\omega=\omega \cup T_{k}$$
7. $$k=k+1, T=T_{k},$$ 如果$$T$$不是由根节点单独组成的树, 则回到步骤2继续递归执行。否则就已经得到了所有的可选最优子树集合 $$\omega .$$
8. 采用交叉验证在 $$\omega$$ 选择最优子树 $$T_{\alpha}$$

### Package ‘rpart’

Recursive partitioning for classification, regression and survival trees: [https://cran.r-project.org/web/packages/rpart/rpart.pdf](https://cran.r-project.org/web/packages/rpart/rpart.pdf)

